# Model
MODEL_DIR: "meta-llama/Llama-2-70b-hf"
ACTS_LAYER: 32

# Encoder Size
PROJECTION_FACTOR: 10

# Save Paths
PROMPT_IDS_PATH: "acts_data/activations_prompt_ids.npy"
ACTS_DATA_PATH: "acts_data/activations_dataset.pt"
ENCODER_PATH: "acts_data/learned_encoder.pt"
BIASES_PATH: "acts_data/learned_biases.pt"
TOP_K_INFO_PATH: "acts_data/token_info.csv"

# Autoencoder Training
LAMBDA_L1: 3.0
LEARNING_RATE: 1.0e-3

# Reproducibility
SEED: 0
